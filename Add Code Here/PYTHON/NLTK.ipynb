{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Not', 'all', 'that', 'Mrs.', 'Bennet', ',', 'however', ',', 'with', 'the', 'assistance', 'of', 'her']\n",
      "['Not all that Mrs. Bennet, however, with the assistance of her']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "sent = \"Not all that Mrs. Bennet, however, with the assistance of her\"\n",
    "print(word_tokenize(sent))\n",
    "print(sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the unclean version: ['Not', 'all', 'that', 'Mrs.', 'Bennet', ',', 'however', ',', 'with', 'the', 'assistance', 'of', 'her']\n",
      "This is the cleaned version: ['Not', 'Mrs.', 'Bennet', ',', 'however', ',', 'assistance']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords        # the corpus module is an \n",
    "                                         # extremely useful one. \n",
    "                                         # More on that later.\n",
    "stop_words = stopwords.words('english')  # this is the full list of\n",
    "                                         # all stop-words stored in\n",
    "                                         # nltk\n",
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "print(\"This is the unclean version:\", token)\n",
    "print(\"This is the cleaned version:\", cleaned_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'play', 'playful', 'play']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "'''\n",
    "This is when ‘fluff’ letters (not words) are removed from a word and grouped together with its “stem form”. \n",
    "For instance, the words ‘play’, ‘playing’, or ‘plays’ convey the same meaning (although, again, not exactly, \n",
    "but for analysis with a computer, that sort of detail is still not a viable option). \n",
    "So instead of having them as different words, we can put them together under the same umbrella term ‘play’.\n",
    "'''\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = ['play', 'playing', 'plays', 'played',\n",
    "         'playfullness', 'playful']\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five daughter , could ask on the subject , wa suffici to draw \n"
     ]
    }
   ],
   "source": [
    "sent2 = \"five daughters, could ask on the subject, was sufficient to draw\"\n",
    "token = word_tokenize(sent2)\n",
    "stemmed = \"\"\n",
    "for word in token:\n",
    "    stemmed += stemmer.stem(word) + \" \"\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SONAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Not', 'RB'), ('Mrs.', 'NNP'), ('Bennet', 'NNP'), (',', ','), ('however', 'RB'), (',', ','), ('assistance', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Tagging Parts of Speech (pos)The next essential thing we want to do is tagging each word in the corpus \n",
    "#(a corpus is just a ‘bag’ of words) we created after converting sentences by tokenizing.\n",
    "\n",
    "from nltk import pos_tag \n",
    "token = word_tokenize(sent) + word_tokenize(sent2)\n",
    "tagged = pos_tag(cleaned_token)                 \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
